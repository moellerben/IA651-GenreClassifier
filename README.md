# IA651-GenreClassifier
### Ben Moeller, Luke Buckler

For this project, we are trying to use machine learning techniques to classify genres of music.

## Data Source
Our data source for this project is the music library of WTSC, Clarkson's FM radio station. With around 15,000 songs in their library, many of which already labeled with genre metadata tags, this made for a perfect data source. In addition, the music library is mapped as a network drive for authorized users, making it super easy to access using Python.

## Data Preparation
In order to classify the audio data, we took the approach of converting audio to image, then using image classification techniques already well studied by the machine learning community. The audio to image conversion was done by generating a spectrogram from the song, which can be visualized by plotting frequency on the vertical axis, time on the horizontal axis, and intensity as brightness of each pixel. Since audio is perceived on a logarithmic scale, the spectrogram was scaled accordingly so a decent range of values is visible.

Since using every sample of every song proved to be way too much data, we opted to take just 5 1-second long clips from each song. We also created a second data file containing 2-second long clips to see how it impacts the accuracy of the model. Since the sample rate and number of frequency bins in the fast Fourier transform are consistent, the dimensionality of each clip is also consistent (for each clip duration).

The label for this process was taken directly from the "genre" metadata field within each file. Audio files that did not have any content in the "genre" tag were ignored. Many publishers and artists have different names for similar genres, so we created a mapping from each genre name appearing in the song tags to a consistent set of 10 genre names. The final dataset contains 8,669 songs with 5 clips each for a total of 43,345 clips. Each clip is stored as an image in a directory structure which indicates duration, train/test, and class.

## ML Techniques
Initially, our plan was to convert the images of the spectrographs into an array of intensities for each pixel, and then flatten that array into a row in a csv for each sample. The problem that we ran into for this was that in order to run the data through a CNN, we had to reshape the arrays in memory for every iteration of the CNN training and this produced many errors and was too taxing for us to run. We then pivoted to keeping the images as images and creating a folder structure storing all the images in folders based on music genre, and then setting up directories for where the training and testing data was. By doing this we were able to make the model training much less computationally difficult. For our model, we tested a number of different CNN architectures, as well as various parameters. We tried decreasing and increasing the batch size, increased the number of epochs but introduced the early stopping mechanic to save time where the difference in accuracy would be very small. As well, we added batch normalization between Conv2D steps to increase the speed of the model training. We also experimented with different optimizers but found adam to be the best performing option. In the end we went with the following model architecture:

Model: "sequential_6"
Layer (type)   

conv2d_18 (Conv2D) -> batch_normalization_12 (BatchNormalization) -> conv2d_19 (Conv2D) -> max_pooling2d_12 (MaxPooling2D) -> conv2d_20 (Conv2D) -> batch_normalization_13 (BatchNormalization) -> conv2d_21 (Conv2D) -> max_pooling2d_13 (MaxPooling2D) -> flatten_5 (Flatten) -> dropout_5 (Dropout) -> dense_6 (Dense) 

## Results
We ended up with a mixed bag of results. Initially we will talk about the 2 second long clips because they are larger images and more likely to have features indicative of genre within them. The biggest obstacle is that one run through of training the model on average took about 70 minutes, which made refining the model a very time consuming task. Many different methods to reduce the training time were attempted but only marginal performance increases were achieved. The other issue we had was model overfitting. As our CNN was going through iterations, the accuracy was growing and the loss for the training set was decreasing which is encouraging, but after a point only marginal increases in the validation accuracy and marginal decreases in the validation loss were observed. Both the validation accuracy and loss started to flip flop between improving and getting worse. This effect is likely due to model overfitting, which is why we added both batch normalization and kernel regularizers in order to prevent the overfitting where possible. There were some improvements on stopping overfitting, but there were still overfitting effects observed in the final model. In the end, for the 2 second samples, we got an accuracy of 0.7160 and a loss of 0.7083 for the training set, and an accuracy of 0.3808 and a loss of 2.3011 for the validation set. The difference in three numbers is large, but an accuracy of ~38% when random chance would leave it at 10% is acceptable. When looking at how successful the model was at correctly guessing each individual genre, we can see that the f1 score for each genre seems to be correlated with the “support” value which just means the number of images in the dataset that belong to that genre. Since only 4 of the genres had greater than 1000 images, they all received the highest f1 scores (>0.16), and genres with very low number of images (<250) received very low f1 scores (<0.04).

When looking at how the model performed on the 1 second long samples, it was quite interesting. The model took just about the same time to run, even though the files are smaller in size. When looking at how the model performed, we ended up receiving lower accuracy and higher loss scores for the training set when compared with the 2 second samples, however the accuracy for the validation set was lower in the 2 second samples but the loss was lower in the 1 second samples. There was much more volatility in the loss and accuracy for the validation sets in the 1 second samples, with both values jumping around quite a bit. It would make sense that the 2 second samples have more distinguishable features which the CNN would be able to pick up on, but there was not a significant improvement in accuracy between the different sample lengths.

Overall, we are happy that the model is considerably better at distinguishing the genre of the sample than random chance, there are still improvements which can be made on these models, primarily, I believe that changing the way the images are loaded in to train the model so that random slices of the images are included could improve the model performance in both computation and reduce the overfitting problems we were experiencing. Another improvement would be to have a much more balanced dataset, with more similar numbers of samples between genres, which was unfortunately not possible with our data, but would likely improve the model’s accuracy.
